from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.messages import BaseMessage, HumanMessage, AIMessage
from langchain_core.documents import Document
from langchain_openai import ChatOpenAI
from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from app.config import settings
from app.models import ChatRequest
import asyncpg
from typing import List


def get_chat_llm(provider: str | None = None):
    """LLM í”„ë¡œë°”ì´ë”ë¥¼ ì„ íƒí•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    llm_provider = provider or settings.default_llm_provider
    
    if llm_provider == 'google':
        return ChatGoogleGenerativeAI(
            model="gemini-2.0-flash",
            google_api_key=settings.google_api_key,
            temperature=0.2
        )
    
    return ChatOpenAI(
        model="gpt-4o",
        temperature=0,
        openai_api_key=settings.openai_api_key
    )


# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
prompt = ChatPromptTemplate.from_messages([
    ("system", """ë‹¹ì‹ ì€ ì ì„±ê²€ì‚¬ ê²°ê³¼ ìƒë‹´ ì „ë¬¸ ì±—ë´‡ì…ë‹ˆë‹¤. 

ğŸš¨ **ì ˆëŒ€ ê·œì¹™** ğŸš¨
1. **ë°˜ë“œì‹œ ì•„ë˜ [ê²€ì‚¬ ê²°ê³¼]ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”**
2. **[ê²€ì‚¬ ê²°ê³¼]ì— ì—†ëŠ” ë‚´ìš©ì€ ì ˆëŒ€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”**
3. **ì¼ë°˜ì ì¸ ì„¤ëª…ì´ë‚˜ ì¶”ì¸¡ì„ í•˜ì§€ ë§ˆì„¸ìš”**
4. **[ê²€ì‚¬ ê²°ê³¼]ê°€ ë¹„ì–´ìˆê±°ë‚˜ ê´€ë ¨ ì •ë³´ê°€ ì—†ìœ¼ë©´ "ê²€ì‚¬ ê²°ê³¼ì—ì„œ í•´ë‹¹ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤"ë¼ê³  ë‹µë³€í•˜ì„¸ìš”**

ğŸ“‹ **ìš©ì–´ êµ¬ë¶„**
- "ì„±í–¥" = [ì„±í–¥] íƒœê·¸ê°€ ë¶™ì€ ë‚´ìš©ë§Œ (ì˜ˆ: ì§„ì·¨í˜•, ì°½ì¡°í˜•, ì œì‘í˜• ë“±)
- "ì‚¬ê³ ë ¥" = [ì‚¬ê³ ë ¥] íƒœê·¸ê°€ ë¶™ì€ ë‚´ìš©ë§Œ (ì˜ˆ: ì°½ì˜ì ì‚¬ê³ ë ¥, ìˆ˜ì§ì ì‚¬ê³ ë ¥ ë“±)
- "ì—­ëŸ‰" ë˜ëŠ” "ì¬ëŠ¥" = [ì—­ëŸ‰] íƒœê·¸ê°€ ë¶™ì€ ë‚´ìš©ë§Œ
- "ì§ì—…" = [ì§ì—…] íƒœê·¸ê°€ ë¶™ì€ ë‚´ìš©ë§Œ

âœ… **ë‹µë³€ ë°©ë²•**
1. [ê²€ì‚¬ ê²°ê³¼]ì—ì„œ ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ íƒœê·¸ë¥¼ ì°¾ìœ¼ì„¸ìš”
2. í•´ë‹¹ íƒœê·¸ì˜ ë‚´ìš©ì„ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”
3. êµ¬ì²´ì ì¸ ì´ë¦„, ì ìˆ˜, ìˆœìœ„ ë“±ì„ ì •í™•íˆ ì–¸ê¸‰í•˜ì„¸ìš”
4. ì¹œê·¼í•˜ê³  ê³µê°ì ì¸ í†¤ì„ ìœ ì§€í•˜ì„¸ìš”

âŒ **ì ˆëŒ€ í•˜ì§€ ë§ ê²ƒ**
- ì¼ë°˜ì ì¸ ì„±í–¥/ì‚¬ê³ ë ¥ ì„¤ëª…ì„ ë§Œë“¤ì–´ë‚´ì§€ ë§ˆì„¸ìš”
- [ê²€ì‚¬ ê²°ê³¼]ì— ì—†ëŠ” ì •ë³´ë¥¼ ì¶”ê°€í•˜ì§€ ë§ˆì„¸ìš”
- ë‹¤ë¥¸ íƒœê·¸ì˜ ë‚´ìš©ì„ í˜¼ë™í•˜ì§€ ë§ˆì„¸ìš”"""),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", """--- [ê²€ì‚¬ ê²°ê³¼] ---
{context}

--- [ì§ˆë¬¸] ---
{question}

ìœ„ [ê²€ì‚¬ ê²°ê³¼]ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì—¬ ë‹µë³€í•˜ì„¸ìš”. ê²°ê³¼ì— ì—†ëŠ” ë‚´ìš©ì€ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.""")
])


class SimpleChatMessageHistory(BaseChatMessageHistory):
    """ê°„ë‹¨í•œ PostgreSQL ê¸°ë°˜ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬"""
    
    def __init__(self, session_id: str, anp_seq: int):
        self.session_id = session_id
        self.anp_seq = anp_seq
        self._messages: List[BaseMessage] = []
    
    async def aget_messages(self) -> List[BaseMessage]:
        """ë¹„ë™ê¸°ë¡œ ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°"""
        conn = await asyncpg.connect(settings.database_url.replace('postgresql+asyncpg://', 'postgresql://'))
        try:
            rows = await conn.fetch(
                """
                SELECT message_type, content 
                FROM chat_history 
                WHERE session_id = $1 AND anp_seq = $2
                ORDER BY created_at ASC
                LIMIT 10
                """,
                self.session_id,
                self.anp_seq
            )
            
            messages = []
            for row in rows:
                if row['message_type'] == 'human':
                    messages.append(HumanMessage(content=row['content']))
                else:
                    messages.append(AIMessage(content=row['content']))
            
            return messages
        finally:
            await conn.close()
    
    async def aadd_message(self, message: BaseMessage) -> None:
        """ë¹„ë™ê¸°ë¡œ ë©”ì‹œì§€ ì¶”ê°€"""
        message_type = 'human' if isinstance(message, HumanMessage) else 'ai'
        
        conn = await asyncpg.connect(settings.database_url.replace('postgresql+asyncpg://', 'postgresql://'))
        try:
            await conn.execute(
                """
                INSERT INTO chat_history (session_id, anp_seq, message_type, content)
                VALUES ($1, $2, $3, $4)
                """,
                self.session_id,
                self.anp_seq,
                message_type,
                message.content
            )
        finally:
            await conn.close()
    
    def add_message(self, message: BaseMessage) -> None:
        """ë™ê¸° ë©”ì„œë“œ (ì‚¬ìš© ì•ˆ í•¨)"""
        raise NotImplementedError("Use aadd_message instead")
    
    def clear(self) -> None:
        """ëŒ€í™” ê¸°ë¡ ì‚­ì œ (ì‚¬ìš© ì•ˆ í•¨)"""
        pass


class CustomRetriever:
    """ì‚¬ìš©ìë³„ í•„í„°ë§ëœ ì»¤ìŠ¤í…€ Retriever"""
    
    def __init__(self, anp_seq: int, language_code: str):
        self.anp_seq = anp_seq
        self.language_code = language_code
        self.embeddings = GoogleGenerativeAIEmbeddings(
            model="models/text-embedding-004",
            google_api_key=settings.google_api_key
        )
    
    async def ainvoke(self, question: str) -> List[Document]:
        """ë¹„ë™ê¸°ë¡œ ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ - í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ + ë²¡í„° ê²€ìƒ‰"""
        import json
        
        # ì§ˆë¬¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ
        question_lower = question.lower()
        filters = []
        
        # í‚¤ì›Œë“œ ë§¤ì¹­ (ëˆ„ì  ì ìš©)
        if 'ì„±í–¥' in question or 'ìœ í˜•' in question:
            filters.extend(['top_tendency', 'top_tendency_explain', 'bottom_tendency'])
            print(f"ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­: 'ì„±í–¥' ê´€ë ¨")
            
        if 'ì‚¬ê³ ' in question or 'ì‚¬ê³ ë ¥' in question or 'ì‚¬ê³ ìœ í˜•' in question:
            filters.extend(['thinking_main', 'thinking_detail'])
            print(f"ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­: 'ì‚¬ê³ ë ¥' ê´€ë ¨")
            
        if 'ì¬ëŠ¥' in question or 'ì—­ëŸ‰' in question or 'ëŠ¥ë ¥' in question:
            filters.extend(['talent'])
            print(f"ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­: 'ì—­ëŸ‰' ê´€ë ¨")
            
        if 'ì§ì—…' in question or 'ì§„ë¡œ' in question or 'ì§ë¬´' in question:
            filters.extend(['suitable_job', 'competency_job', 'duty'])
            print(f"ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­: 'ì§ì—…' ê´€ë ¨")
            
        if 'í•™ìŠµ' in question or 'ê³µë¶€' in question:
            filters.extend(['learning_style'])
            print(f"ğŸ” í‚¤ì›Œë“œ ë§¤ì¹­: 'í•™ìŠµ' ê´€ë ¨")
            
        # ì¤‘ë³µ ì œê±°
        chunk_type_filter = list(set(filters)) if filters else None
        
        # ì„ë² ë”© ìƒì„± (í•„í„°ë§ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ì •ë ¬ì„ ìœ„í•´ í•„ìš”)
        question_embedding = self.embeddings.embed_query(question)
        embedding_str = '[' + ','.join(map(str, question_embedding)) + ']'
        
        # PostgreSQLì—ì„œ ê²€ìƒ‰
        conn = await asyncpg.connect(settings.database_url.replace('postgresql+asyncpg://', 'postgresql://'))
        try:
            if chunk_type_filter:
                # í‚¤ì›Œë“œ í•„í„°ë§ + ë²¡í„° ìœ ì‚¬ë„ ì •ë ¬
                rows = await conn.fetch(
                    """
                    SELECT content, metadata, chunk_type
                    FROM report_chunks
                    WHERE anp_seq = $1 AND language_code = $2
                      AND chunk_type = ANY($3)
                    ORDER BY embedding <=> $4::vector
                    LIMIT 15
                    """,
                    self.anp_seq,
                    self.language_code,
                    chunk_type_filter,
                    embedding_str
                )
                print(f"ğŸ“Š í‚¤ì›Œë“œ í•„í„°ë§+ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(rows)}ê°œ ì²­í¬ ê²€ìƒ‰ë¨")
            else:
                # ìˆœìˆ˜ ë²¡í„° ê²€ìƒ‰ (í•„í„° ì—†ìŒ)
                rows = await conn.fetch(
                    """
                    SELECT content, metadata, chunk_type
                    FROM report_chunks
                    WHERE anp_seq = $1 AND language_code = $2
                    ORDER BY embedding <=> $3::vector
                    LIMIT 15
                    """,
                    self.anp_seq,
                    self.language_code,
                    embedding_str
                )
                print(f"ğŸ“Š ì „ì²´ ë²¡í„° ê²€ìƒ‰ ê²°ê³¼: {len(rows)}ê°œ ì²­í¬ ê²€ìƒ‰ë¨")
            
            # ë””ë²„ê¹…: ê²€ìƒ‰ëœ ì²­í¬ íƒ€ì… ì¶œë ¥
            print(f"\n{'='*60}")
            print(f"ğŸ” ì§ˆë¬¸: {question}")
            print(f"ğŸ¯ ì ìš©ëœ í•„í„°: {chunk_type_filter if chunk_type_filter else 'ì—†ìŒ (ì „ì²´ ê²€ìƒ‰)'}")
            print(f"{'='*60}")
            for i, row in enumerate(rows, 1):
                content_preview = row['content'][:100].replace('\n', ' ')
                print(f"{i}. [{row['chunk_type']}] {content_preview}...")
            print(f"{'='*60}\n")
            
            # Document ê°ì²´ë¡œ ë³€í™˜
            documents = []
            for row in rows:
                # metadataê°€ ë¬¸ìì—´ì´ë©´ JSON íŒŒì‹±
                metadata = row['metadata']
                if isinstance(metadata, str):
                    try:
                        metadata = json.loads(metadata)
                    except json.JSONDecodeError:
                        metadata = {}
                elif metadata is None:
                    metadata = {}
                
                documents.append(
                    Document(page_content=row['content'], metadata=metadata)
                )
            
            return documents
        finally:
            await conn.close()
    
    def invoke(self, question: str) -> List[Document]:
        """ë™ê¸° ë©”ì„œë“œ (ì‚¬ìš© ì•ˆ í•¨)"""
        raise NotImplementedError("Use ainvoke instead")


def format_docs(docs: List[Document]) -> str:
    """ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸ë¥¼ ë¬¸ìì—´ë¡œ í¬ë§·"""
    return "\n\n".join(doc.page_content for doc in docs)


async def get_rag_chain_with_history(request: ChatRequest):
    """LangChain LCEL ìŠ¤íƒ€ì¼ì˜ RAG ì²´ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤."""
    from langchain_core.runnables import RunnableLambda, RunnableParallel
    
    llm = get_chat_llm(request.provider)
    retriever = CustomRetriever(request.anp_seq, request.language_code)
    history = SimpleChatMessageHistory(request.session_id, request.anp_seq)
    
    # ëŒ€í™” ê¸°ë¡ ê°€ì ¸ì˜¤ê¸°
    messages = await history.aget_messages()
    
    # LCEL ì²´ì¸ êµ¬ì„±
    # 1. ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰
    retrieve_chain = RunnableLambda(lambda x: retriever.ainvoke(x["question"]))
    
    # 2. ë¬¸ì„œ í¬ë§·íŒ…
    format_chain = RunnableLambda(lambda docs: format_docs(docs))
    
    # 3. í”„ë¡¬í”„íŠ¸ + LLM
    async def generate_answer(inputs):
        # ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ë° í¬ë§·
        docs = await retriever.ainvoke(inputs["question"])
        context = format_docs(docs)
        
        # í”„ë¡¬í”„íŠ¸ ìƒì„±
        final_prompt = prompt.format_messages(
            chat_history=messages,
            context=context,
            question=inputs["question"]
        )
        
        # LLM í˜¸ì¶œ
        response = await llm.ainvoke(final_prompt)
        return response.content if hasattr(response, 'content') else str(response)
    
    # ì²´ì¸ ì‹¤í–‰
    answer = await generate_answer({"question": request.question})
    
    # ëŒ€í™” ê¸°ë¡ ì €ì¥
    await history.aadd_message(HumanMessage(content=request.question))
    await history.aadd_message(AIMessage(content=answer))
    
    return answer
